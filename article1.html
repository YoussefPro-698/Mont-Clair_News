<html>
    <head>
        <meta charset="utf-8">
        <title>Mont-clair News</title>
        <meta name="description" content="decouvrer les nouvelles sur la technologie et aussisur l'IA">
        <link rel="stylesheet" href="journal.css">
    </head>
    <body>
        <div class="cover">
            <a href="news.html"><img src="logo.png" class="cover"></a>
            <div align="center">
                <p class="title">
                    Des chercheurs ont découvert que le modèle d'IA GPT-4 d'OpenAI est capable de pirater des sites web et de voler des informations dans des bases de données en ligne sans aide humaine
                </p>
            </div>
        </div>
        <div align="center">
            <p style="margin-top: 70px"><b>Des chercheurs ont démontré que les grands modèles de langage sont capables de pirater des sites web de manière autonome, en effectuant des tâches complexes sans connaissance préalable de la vulnérabilité. Le modèle GPT-4 d'OpenAI pouvait pirater 73 % des sites web lors de l'étude. Cette étude rappelle la nécessité pour les fournisseurs de LLM de réfléchir soigneusement au déploiement et à la publication des modèles.</b></p>
            <p>Les modèles d'IA, qui font l'objet de préoccupations constantes en matière de sécurité concernant les résultats nuisibles et biaisés, présentent un risque qui va au-delà de l'émission de contenu. Lorsqu'ils sont associés à des outils permettant une interaction automatisée avec d'autres systèmes, ils peuvent agir seuls comme des agents malveillants.</p>
            <img src="https://www.lavieeco.com/wp-content/uploads/2023/04/image-of-hand-holding-an-ai-face-looking-at-the-words-chatgpt-openai.jpg">
            <p style="margin-top: 30px">La démonstration effectuée par des informaticiens affiliés à l’Université de l'Illinois Urbana-Champaign (UIUC) révèle l'usage potentiellement néfaste des grands modèles de langage (LLM) dans le domaine de la cybersécurité. Ces chercheurs, dont Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan et Daniel Kang, ont publié un article intitulé "LLM Agents can Autonomously Hack Websites" (Les agents LLM peuvent pirater des sites web de manière autonome), dans lequel ils exposent en détail leurs découvertes.</p>
            <img style="border-radius: none" src="https://lejournalduhack.com/wp-content/uploads/2024/03/image-8-1140x373.png">
            <p style="margin-top: 30px">L'étude démontre que des agents LLM autonomes, dotés de fonctionnalités avancées telles que l'accès aux API, la navigation web automatisée et la planification basée sur le retour d'information, peuvent s'introduire dans des applications web vulnérables sans intervention humaine. Ces agents sont capables d'effectuer des attaques sophistiquées, comme les attaques SQL Union, sans nécessiter une connaissance préalable de la vulnérabilité spécifique. Le modèle le plus performant, le GPT-4, a réussi à pirater 73,3 % des vulnérabilités testées.</p>
            <p>L'article souligne également que ces capacités de piratage autonomes sont rendues possibles grâce aux avancées récentes dans les LLM, notamment la capacité à interagir avec des outils via des appels de fonction et à lire des documents. De plus, ces fonctionnalités sont désormais largement disponibles dans les API standard, facilitant leur implémentation avec seulement quelques lignes de code.</p>
            <img style="border-radius: none" src="https://lejournalduhack.com/wp-content/uploads/2024/03/image-9.png">
            <p style="margin-top: 30px">Une analyse des coûts associés à ces piratages autonomes révèle un coût moyen d'environ 9,81 dollars par tentative, ce qui est significativement inférieur au coût de l'intervention humaine dans le processus de piratage.</p>
            <h1>Conclusion :</h1>
            <p style="margin-top: 30px">Cette recherche montre que les agents LLM peuvent pirater des sites web de manière autonome, sans connaître la vulnérabilité à l’avance. L’agent le plus performant peut même trouver de manière autonome des vulnérabilités dans des sites Web du monde réel. Les résultats montrent en outre des lois d’échelle fortes avec la capacité des LLM à pirater des sites web : GPT-4 peut pirater 73 % des sites web construits pour l’étude, contre 7 % pour GPT-3.5 et 0 % pour tous les modèles open-source. Le coût de ces piratages par des agents LLM est probablement beaucoup moins élevé que le coût d’un analyste en cybersécurité.</p>
            <p>Combinés, ses résultats montrent la nécessité pour les fournisseurs de LLM de réfléchir soigneusement au déploiement et à la publication des modèles. On peut souligner deux résultats importants. Tout d’abord, l’étude constate que tous les modèles open-source existants sont incapables de pirater de manière autonome, mais que les modèles frontières (GPT-4, GPT-3.5) le sont. Deuxièmement, les chercheurs pensent que ces résultats sont les premiers exemples de dommages concrets causés par les modèles frontières. Compte tenu de ces résultats, ils espèrent que les fournisseurs de modèles open source et fermé examineront attentivement les politiques de diffusion des modèles frontières.</p>
            <h1>Journal de Mont-Clair, 30/3/2024</h1>
            <h2>Par Youssef Lmouden</h2>
        </div>
    </body>
</html>